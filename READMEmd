# Set-up

## clone the repository

```bash
```

## configure the interpreter
```bash
poetry install
```

## download the dataset
Install the Kaggle API by running the following command in your terminal:

```bash
poetry add kaggle
```

create a Kaggle API token by following the instructions [here]():

download the dataset by running the following command in your terminal:

```bash
cd data || exit
kaggle competitions download -c leap-atmospheric-physics-ai-climsim
unzip leap-atmospheric-physics-ai-climsim.zip
```


# Big Questions:
- how to handle the very large datasets? Options: cudf, dask, spark, polars
- eda: what is going on?
-

# Data Notes
- The dataset (both training and test) for this competition is generated by the
(E3SM-MMF) climate model.
- The multi-scale nature of E3SM-MMF allows it to explicitly resolve the
effects of small-scale processes such as clouds and storms on large-scale
climate patterns.
- However, this multi-scale framework comes at a great computational cost,
limiting its usage for experiments and ensemble climate projections.

- Every row of the training set corresponds to the inputs and outputs of a
cloud-resolving model (CRM) in E3SM-MMF at some location and timestep.
- There are **556 columns** corresponding to **25 input variables** and
- **368 columns** corresponding to **14 target variables**.
- Some variables (like air temperature) span an entire atmospheric column and
have 60 vertical levels, and other variables (like precipitation) are scalars.
- For the vertically resolved variables, an "_" followed by a number in the
range [0,59] is appended to denote vertical level.
- Lower numbers denote higher positions in the atmosphere.

# Objective
- The goal is to train a model to emulate the effects of these small-scale
processes at a fraction of the cost of explicitly resolving them
- Your goal is to create a model that predicts the target variables associated
with a given set of input variables.